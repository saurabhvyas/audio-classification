good morning everyone for those of you

in my time zone good afternoon for those

of you in New York all right so today

I'm going to give you a talk about

practical methodology for deploying

machine learning this is a talk that's

sort of an homage to talk by my Master's

adviser Andrew who was a professor at

Stanford at the time he did the original

version of this talk today I'll be

presenting my own take on it and some

updated advice based on the last few

years of advances in machine learning

and in terms of the format of the talk

I'll be able to answer a few questions

live during the talk but for the most

part if there's a lot of questions I'll

need to postpone them until the very end

but if there's just one or two quick

clarification things about the slides

feel free to jump in and as long as

there's a low volume of questions I'll

answer them as we go the basic idea

behind this talk is that I want to help

focus your attention on what drives

success in machine learning I understand

a lot of people at this conference are

just beginning their journey of learning

how to work in AI and how to use machine

learning the thing is a lot of tutorials

and text books and videos about machine

learning don't really focus you on

exactly what will lead to the fastest

improvement in your ability to function

in the field most tutorials that you

read proceed by amassing several

different algorithms inductive

principles inference techniques and so

on they make it seem like the way to

become an expert in machine learning is

to know exactly how every algorithm from

the relevance vector machine to Bayesian

linear regression works so I'd say over

here this is one of the things that

people seem to think drive success in

machine learning it's always nice to

know about more algorithms but I would

argue it's far more important to have

lots of data and to know how to apply

three to four standard techniques when I

say know how to apply I mean to know how

to configure their settings understand

whether they're working well

we're working poorly and understand how

to adjust and react if you find that

they are working poorly so this talk

today focuses on this third topic how to

apply the bread-and-butter techniques of

machine learning as a case study that

I'll use as a running example to help

give you concrete instances of the

different advice that I apply I'll be

telling you about a system that I helped

build at Google this is the Street View

address numbered transcription system

the basic idea is that Google Maps can

give you directions to many different

buildings and addresses around the world

but we don't necessarily have all the

buildings you might want to go to in our

Maps already one way that we can add

more buildings to our maps is to use the

GPS and photo data from the Street View

cars as the Street View cars drive

around different streets they take

photos of everything they see and they

also record the GPS coordinates of their

location when they took the photo so if

we see the address number of a building

and that building isn't already on our

map we could in principle put that

building into Google Maps at the correct

GPS coordinates the problem is there are

an awful lot of address numbers that we

take photos of and it's labor-intensive

to transcribe them and put them on the

map the way we solve this problem was we

built a neural network to transcribe the

numbers out of the photographs and

convert them into a computer readable

digital format so that we could then

update the maps entries throughout this

talk I'll refer to a few different

challenges that we encountered along the

the progress of this project and I'll

describe the way that we overcame each

of these challenges as part of my

general advice for you to apply in your

own work the basic methodology I'm going

to advocate is a three step process in

the first step you identify the needs

that you have for the product or the

service or the research project that

you're building you then use these needs

to figure out what your goals should be

your goals should be specific and

quantitative part of the planning

to choose what metrics you will use to

measure success and exactly what kind of

numbers you need to achieve for those

metrics after you've set up your goals

you need to build an end-to-end system

as soon as possible you don't want to

over engineer your project before you

begin working on it just think of the

simplest thing that can actually produce

a usable output for the task that you

want to solve it doesn't matter if the

output is actually any good or not it

just needs to be something that can

actually be scored using your metrics

once you have the end-to-end system in

place you begin step three data driven

refinement in step three you measure

different properties of your system you

figure out which aspects of it are

working well and which aspects are

working poorly and you use these data

driven metrics to improve the parts that

are working poorly and improve them I

haven't uploaded the slides sorry I can

upload them right after the talk after

you have improved your system with this

data-driven refinement process you might

end up with something that's complex at

the end but it won't be over engineered

it will be engineered to be just as

complex as you need to solve the task if

you design a complicated system before

you've actually started tackling the

problem you might overestimate how

complicated it needs to be and end up

with something that's harder to maintain

and use in your business than you need

all right so that's the overall

three-step process now I'll go through

each of these steps in a little bit more

detail the first step is identifying the

needs of your product or service and

defining metric based goals that will

ensure you meet those needs one thing

you need to think about is how could how

much accuracy you actually need in your

product obviously everyone would always

like to build a machine learning system

that's absolutely perfect but that's

usually going to be extremely expensive

if it's even possible at all so you need

to think about how much time and money

you're going to invest to get different

levels of accuracy and you probably want

to initially plan

for the lowest amount of accuracy that's

actually usable in your application some

applications intrinsically demand lots

and lots of accuracy if you're building

a surgery robot that's going to cut

apart people's veins and stitch them

back together

any machine learning you need in there

needs to be extremely accurate

otherwise you're going to cause lots of

pain and suffering and injury if you're

building a mobile app where you take a

picture of your friends that it says

which celebrity they look like there's

not even really a right answer in that

case so it's alright if you don't

necessarily have the most accurate

machine learning system of all time most

applications fall somewhere in the

middle a lot of a lot of business

applications where you use predictions

to plan investments in sales and

acquisitions and so on they fall into

this realm where if you can make better

predictions than the humans are already

making they'll make money we at least

save money relative to what you do if

you operate it off of the human

predictions and the more accurate your

predictions are the better you'll do but

just getting past the human baseline is

sufficient to have a useful product for

the street view address number

transcription system we determined that

we wanted to have human level accuracy

in our transcription we wanted to make

sure that we weren't getting any address

numbers wrong compared to what human

transcribers would get the reason is

that it's very frustrating if you ask

for directions and then Google Maps

leads you to the wrong address

so we needed to make sure that we didn't

reduce the accuracy below the system

that we were already using in order to

measure the accuracy we were able to

just use the percentage of examples that

are correct that's a relatively

straightforward metric but there's other

tasks where you don't want to just

measure the number of examples that are

correct for example if you're building a

test to determine whether someone has a

rare disease you can just say nobody has

the disease because the disease is rare

you'll get a very high accuracy that way

suppose that only 1/10 of 1% of all

people in the population

have this disease you'd get 99.9%

accuracy for cases like that there are

other metrics that you should use

so precision tells you the number of

detections that are correct in the test

for the disease if you say someone has a

disease precision is the probability

that you're correct that they actually

have the disease another metric is

recall the percentage of positive

examples that you actually detect so in

the disease example this would mean of

all the people who have the disease what

what percentage of them do you correctly

diagnosed as having it for the Street

View application this didn't actually

matter because our different classes

that we were categorizing into you were

the six digit numbers that addresses can

take on there's not any one class that's

extremely rare and another class that's

extremely common one metric we did end

up using that was important though and

that you don't see very often and that I

think should be used more often is

coverage so you probably have a part of

coverage before the idea behind coverage

is you can have a machine learning

system that refuses to classify some

inputs basically if you show it an

example where it's not confident what

the correct answer is it can say I am

NOT confident what the correct answer is

you should not use machine learning to

solve this particular input coverage is

the percentage of examples that the

machine learning system actually

confidently classifies and using this

metric was key to our success on the

street view house number pipeline it's

very difficult to reach human level

accuracy on address number transcription

if you insist

that the network classifies every single

example that you show it but you can

easily reach human level accuracy if

you're willing to reduce coverage if

you're willing to say I'm going to throw

out 6 percent of the examples so that I

can reach 99.5% accuracy on the

remaining ones then you can actually get

as high of accuracy as you want provided

that you're willing to throw out more

and more examples so when we set our

goals for the street view house number

transcription pipeline we set our goals

in terms of accuracy and coverage we

wanted to get human level accuracy at a

specific

level of coverage that we felt was

necessary to justify the investment in

the machine learning pipeline there are

other applications besides

classification out there for example if

you're using regression you need to use

a metric based on the size of the

prediction errors that you make like

mean squared error or mean absolute

error all right so the second stage of

the methodology is where you actually

build your end-to-end system you want to

get your system up and running as soon

as possible so that you can identify

what the real challenges are a lot of

the time the aspects that are

challenging are not the ones that you

thought they would be before you started

building it everything and so you want

to find out what's actually going to be

difficult as soon as possible I'm

advocating building the simplest viable

system first the question is what's the

simplest viable system and and where

should you start what should you use as

your beginning point from which you

start to iterate one thing that you can

do if you're working on an application

that's already been done before by other

people is just to find a published

result in this same field and copy the

state-of-the-art method you might not

want to use literally the method that

gets the absolute best accuracy you

might want to get as simple and easy to

implement one that's very close to the

best accuracy if you don't know of any

existing publications that solve the

same application as you then you

probably need to make a judgment call

about what a relatively standard

algorithm is that you should start with

so now I'm going to give you some

guidance about what some reasonable

baselines are to begin with one of the

first questions to ask today in 2015 is

whether you should use deep learning or

not deep learning is very hot right now

and it's my specialty so you're probably

expecting me to tell you should always

use deep learning all the time

that's actually not the case one of the

first things you should decide is

whether the problem you're tackling

requires deep learning or not many tasks

have lots of noise and very little

structure in them usually if this is the

case you don't want deep learning

something like linear regression will

suffice

if there's relatively little noise in

your problem setup

there's a lot of complex structure then

you can use deep learning so when I say

that something has complex structure I

mean you have a task like looking at an

image or a video or a paragraph and

summarizing what it means to a human

being saying you know this paragraph

contains positive sentiment or negative

sentiment or this photo contains an

airplane those involved really

complicated mappings from individual

pixels or individual characters or

individual words to very high-level

abstract ideas but if your system has a

lot of noise and very little structure

and you're just saying something like

here is a house and I'm telling you how

many square feet it has please tell me

how much it costs then there aren't very

many different variables there the

relationship between them is not very

complicated you can solve that with a

much simpler machine learning system the

best shallow baseline to use in one of

these high noise low structure

situations is to use the system that

you're most familiar with a lot of

people will debate endlessly which

machine learning algorithm is the best

but there are theorems out there that to

actually tell you that there is no best

machine learning algorithm usually

you're going to perform the best if you

use something that you're comfortable

with and that you understand if you

understand logistic regression really

well and you know how to tweak it then

use logistic regression if you tweak

logistic regression effectively you will

do much better than if you use an SVM

and don't tweak it well also the same

the same applies in Reverse if you're

familiar with support vector machines

and you think you understand how to

tweak those then by all means begin to

support vector machines before about

2013 before deep learning became really

effective boosted decision trees were

one of my favorite default algorithms if

you have a good implementation of those

and you feel comfortable using them then

they're a really good baseline in a lot

of cases and I successfully use those

for a lot of robotics problems okay so

suppose that you have a very complicated

problem and you have enough data to fit

it in that case then you want to go

ahead and use deep learning

so what deep learning model should you

use the basic way that you decide what

kind of deep learning model to use is

well first as I said on the earlier

slide if there's already a published

baseline that works well in this task

then just copy the architecture from the

published baseline but if you're working

on a totally new problem and you need to

make your own decision about what to use

the main way you decide is you look at

what kind of structure there is in the

data some data doesn't really have any

structure at all it's just a list of

measurements and you just in this case

you could just apply a fully connected

neural network to it there's no special

structure in the neural network at all

it's just completely specified matrices

at every level a lot of tasks have

spatial structure to them this is things

like images or videos or data based on

on maps or any kind of thing we have

sensors aligned on a grid in that case

you can use a convolutional network a

convolutional network is a kind of

neural network that says it's going to

apply the same little function at every

different point in space and if you

learn that one little function really

well then you can apply it everywhere

you don't need to independently relearn

it at every location in the grid finally

if you have a kind of data that has

sequential structure to it then you want

to use a recurrent neural network this

is if you have something like text where

you want to read a long sentence and

then at the very end you're going to be

asked a question about it

so at the end of the sentence you need

to remember something from very early at

the start of the sentence you might

notice that there's a little bit of

overlap between sequential and spatial

structure you can kind of think of time

and space as interchangeable so there is

a little bit of a judgement call of it

whether you want to use convolutional

orbit current networks in some sense

they are similar things I in recurrent

networks kind of imply that you are

convolving over time but that's that's

maybe a little bit more advanced than I

need to get into right now you could do

reasonably well with the choice of

either a convolutional or a recurrent

Network any time that you have this kind

of structure available okay so

you've decided that you want to use a

fully connected neural network to solve

some kind of unstructured data

processing task what's a good baseline

for that situation as of 2015 I would

say that the best baseline to start with

the one that's really easy to implement

and works well in a wide variety of

settings is the two to three hidden

layer feed-forward neural network these

are also called multi-layer perceptrons

and you can go ahead and add more hidden

layers later if you decide that they're

they're needed but to begin with just

try two three maybe even just one hidden

layer you definitely want to use

rectified linear units as your baseline

don't use sigmoids sigmoids are

considered out-of-date now and rectified

linear units are far easier to use you

also probably want to use dropout

geoffery Hinton's regularization

strategy where you randomly mask out

half of the units on every step of

training to train the neural network you

want to use stochastic gradient descent

and momentum this technique is really

effective it works very well as long as

you have a few thousand examples per

class and it's been applied to

everything from speech to vision to

natural language processing it's it's

really the standard engine that drives

pretty much everything in defining right

now part of the reason I'm doing this

talk and highlighting this as an example

of a great baseline for right now is

that it can be hard to sift through the

literature and determine what the latest

advice is so a lot of people are first

getting into deep learning read that

deep learning is all about deep belief

networks and deep bolts and machines and

so on or auto-encoders

I don't really recommend those methods

right now those were performing very

well from about 2006 to 2012 but they're

complicated and difficult to make work

you need to understand a lot more ideas

to get them to work well and these days

there are very few tasks where they're

actually the state-of-the-art anymore

unless you know for a fact

something like auto-encoders is

necessary to perform well on the

application you're working on you

probably want to default to this

backpropagation based rectified linear

network so that's what you do if your

data doesn't have any particular

structure to it if your data has image

structure then you want to use a

convolutional network so I also have

some pretty strong recommendations for

what to use if you're in the

convolutional setting if you're able to

do it I suggest using an inception

Network trade with batch normalization

you'll read a lot of papers out there

about all these different tricks to

train very deep convolutional networks

and in a lot of cases it turns out that

you can actually train as deep of a

network as you want just by using this

batch normalization algorithm that was

released by my colleagues at Google

earlier this year inception and batch

normalization are somewhat complicated

and not every library offers these kinds

of networks so if you're using a library

that doesn't support for example the

inception Network you can fall back to a

simpler convolutional Network once again

I recommend using rectified linear units

the same as in the feed-forward Network

case so just use a convolutional net

work with rectified linear units regular

as it with dropout and train with

stochastic gradient descent in momentum

alright finally if you have a task with

sequential structure then you can use a

recurrent Network in this case the

standard baseline I recommend is the LST

M developed by step lock writer and your

instrument youever um oh yeah I see

somebody's asking about unsupervised

learning if if your task really is

unsupervised like if your final goal is

not to do classification then go ahead

and use an unsupervised learning

algorithm um it depends which kind of

unsupervised task you want to solve if

you're if you're doing something like

denoising then use a denoising

auto-encoder if you're doing something

like generating new samples then you

might want to use a generative

adversarial network or a variational

auto encoder um let me read the

follow-up

oh yeah hidden Markov models hidden

Markov models are

perfectly fine as long as you don't need

the hidden state variable to be

complicated in high dimensional if

there's a very high dimensional hidden

state than hidden Markov models don't

scale as well as recurrent Nets

okay so popping back to the recurrent

Network situation I would recommend the

LST M as the default model that people

use when tackling a complicated AI

complete sequence modeling task you can

train this in stochastic gradient

descent I've heard a lot of people say

that you don't even need to use momentum

in this case the LST M already makes it

easy enough to optimize that you don't

need that extra step but momentum is

usually helpful one thing that's really

important when training any kind of

recurrent network is to apply gradient

clipping that means that during back

propagation as the gradient flows

backward through the network you impose

some maximum size on the gradient and if

the true gradient exceeds that sides you

just you clip it to be smaller than then

what the math actually says it should be

the reason for this is that when you

propagate backward through several

hundred steps in an LST M it can make

the gradient become quite large and a

very large gradient can actually do a

lot of damage if you take a gigantic

gradient step so just by just by

clipping it you can avoid that

instability problem so I see there's a

question about whether google has any

new methods to parallelize llst m as

well I don't actually know anything

about the pyramid lsdm myself so one

thing is at Google we do train a lot of

things using asynchronous gradient

descent where we just train many copies

of the LST m simultaneously that's using

our internal disbelief library um that's

not really something that people outside

can just grab and use but there are

other multi replicas learning strategies

that have been made public one last

trick that you should probably use in

your baseline for a recurrent Network is

setting the forget but get the forget

gate bias to be high that makes it so

that the forget good of the LS TM

initially says not to forget anything

and that helps to make sure

that information flows through the

network originally all right so stage 3

of this methodology pipeline is to do a

data driven adaptation after you've got

your baseline in pit in place you choose

what to do next based on data I'm an

important thing in this step is to not

believe hype about different kinds of

algorithms out there every week there

are dozens of papers appearing in

archives saying you know we have this

new like lingerie in Bayesian

variational variant of this algorithm

you're already using and if you use this

it's going to be a million times better

most of the time most of these papers

are just one author trying to get

attention for their own method and try

to build their resume it takes quite a

long time for an algorithm to get

established and for you to really know

that it's trustworthy so filter out a

bit of the noise try to take a bit of a

conservative approach to what algorithm

to use and expect most of the benefit

you get from doing very bread-and-butter

stuff where you adapt the settings for

the algorithm you're already using or

you gather more data and when you do

change from one algorithm to another it

shouldn't be just because you've read

that some tool is the hottest new thing

it should be because you've used a

metric to figure out what the best next

step should be the most important

metrics are the training error and the

tester you measure how well you're doing

on the training set and you measure how

well you're doing on the test set if

you're not doing well enough on the

train set then you're under fitting and

if you're not doing well on the test set

then you're overfitting so I'll talk a

little bit about how to address each of

these problems if you have high training

error there's a few different steps you

should take I'm most most textbook

advice will immediately start telling

you how to do things to your machine

learning model but I actually say the

very first thing you should do is check

whether your data has a problem make

sure that hasn't been collected poorly

or something like that

if the data has defects then your

algorithm won't be able to fit it next

you should actually inspect your

software for defects and specifically

I'd say don't make your own software

unless you definitely know

you're doing using other established

software that's been tested and improved

by many people is a good way of being

relatively confident that your high

training or doesn't come from a bug once

you're sure that both the data and the

algorithm are correctly gathered and

correctly implemented you can begin to

address high training error by adapting

the learning rate and adapting the other

settings that affect the optimization

procedure you can also make your model

bigger so that it can fit a larger

training set I'll give you a few

examples of each of these things here so

for the Street View transcriber one of

the biggest things we did was we changed

the way that we cropped the photos that

were provided to the system after we had

built our baseline we started looking at

which examples it was getting run over

here on the left here's an example of a

kind of mistake that it was making we

would see these photos that say six six

two four and the correct answer is two

six six two four so we realized that our

automatic cropping system was actually

cropping a little bit too tight we

solved that just by telling it to make

whiter crops and now you can actually

see the complete digit the complete

address number so this is a really

simple and dumb mistake and it's very

simple to fix it doesn't require any

real knowledge of machine learning to

fix all we did was look at what mistakes

were happening and then try to

categorize them and figure out what the

largest bottleneck was so that was our

biggest change that we did in terms of

how much it improved our performance was

we found that sometimes the crop was too

tight and we were misclassifying

examples because of the crop our biggest

change during the development of the

pipeline was not to change algorithms

introduced unsupervised learning

anything like that it was just to

measure where the mistakes were

happening and make a very simple

common-sense change you can also fit the

training set better by increasing the

size of the model here we have a graph

of how our important our performance

improved as we added more layers to this

tree tree transcription system this is

pretty simple we just kept adding more

hidden layers and the performance kept

going up a lot of the time if you have a

lot of data you'll find that you can

keep making the model bigger and bigger

and you always see an improved

a lot of the time you'll end up finding

that your performance is limited by the

size of the model that you're able to

afford to run okay so that's what you do

if you have high training center what if

you have high test set error one thing

you can do is you can dupe data set

augmentation where you make multiple

copies of your training examples

transformed in different ways that's a

relatively cheap way of getting more

data you can also just pay to get more

data if you aren't using a

regularization strategy like drop out

already then you can add drop out and

hope that it that reduces your test set

error so here's a graph of what happens

as you increase your training set size

on the right I'm plotting the optimal

size of the model so as you get more and

more training examples you'll need to

you'll need to use larger and larger

models on the left I'm showing a few

different things that happen one of

these is the training set error the

optimal model size most of the time in

deep learning your optimal model is

going to get zero training error and the

question is just how badly does Nover

fit what is the gap between train and

test performance you'll see that test

performance drops over time as the

training number of training examples

increases so a lot of the time the size

of your data set is the most important

factor driving your success so that's

the end of my talk if you'd like more

information feel free to look at the

deep learning textbook that I'm writing

along with Joshua Ben geo and Aaron

kohrville it's online at Goodfellow

github that I owe and the subdirectory

is DL book so I'm still here for another

20 minutes for questions I'll go ahead

and turn on the camera all right so I

see somebody asks is there any

interactive tool to choose the best AI

algorithm the there is no one best AI

algorithm there's a theorem called the

universal or sorry the no free lunch

theorem it says that if you average over

all possible problems all machine

learning algorithms perform equally well

according to one performance metric so

that means usual

what you need to do is find the best

machine learning algorithm for the task

that you are trying to solve rather than

find the one single best machine

learning algorithm

I see somebody has suggested the Auto ml

challenge I'm not familiar with that one

in particular there are a lot of

contests out there there are a lot of

contests hosted on Kaggle comm for

example so you can look and see if

Kaggle comm has hosted a contest on a

subject that's related to the one you're

trying to solve and if they have it you

can also pay them to host your own

contest if you gather a training set and

so on alright I see somebody has asked

in most supervised problems there is a

predefined number of classes to choose

from and it is often hard to define and

learn an appropriate set of training

data that does not belong to those

classes eg the non-faces for face

detection or the unknown subjects and a

face recognition system are there better

ways to deal with this problem rather

than just training with huge sets of non

class data so in in that case I guess

you're saying you want to learn to

reject things that are not positive

examples one thing you can do is you can

just synthetically generate non class

data another thing you can do is you can

use Bayesian methods that automatically

reduce the probability of specific

classes if they're far from anything

you'd seen in the training set so like

if you use some kind of Gaussian process

regression it's not really going to have

low confidence if the input doesn't

resemble something it was in the

training set provided that you have some

kind of RBF kernel I see somebody asks

is there a limit for the size of data

set at different deep learning

algorithms can handle I know there

really isn't one of the great things

about deep learning is that it's

trainable using stochastic gradient

descent so you just load mini batches of

examples and the cost of each step of

training doesn't really change based on

the size of the whole training set it

only changes based on the size of the

mini batch that you use so you can train

with a trillion examples and use a mini

batch of size 100 and you'll do

perfectly fine that's one of the main

advantages of deep learning over kernel

machines

if you use something like a colonel eyes

svm you usually need to build a gram

matrix where it's size is the number of

rows is equal to the training set size

and the number of columns is also equal

to the training set size so you get

quadratic scaling in memory and time

with training set size deep loading

completely avoids that problem somebody

asks our neural networks still better

for unstructured data I would say it

depends on what exactly you mean by

unstructured if you mean unstructured in

the sense that nobody has curated it and

divided it into different topics and

fields that have been specifically

labeled then I'd say neural networks are

probably a pretty good bet do I know of

any library that can be used in android

I don't off the top of my head at least

not anything that's that's released

publicly but I know different companies

have things internally have you seen the

tool K Xen that's just the model based

on the data you have I have not seen

that tool it is conceivable that you

could build a classifier that tells you

which tool to use though somebody asks

what is an average day at Google like

for me ok so everybody at Google has a

very different schedule really so

anybody you ask is going to tell you

something very different um I don't

manage anybody so I don't actually spend

any of my time on managing people if you

asked for example my manager or his

manager what their day is like they'd

spent a lot of time meeting with their

reports I don't do that unless I have an

intern um a lot of the time I go to

several different meetings for different

projects and I help people figure out

what machine learning algorithm they

should be using or if I figure out if

they need me to invent some kind of new

technique for them to use for their

project I spend an hour - every day

working on the deep learning textbook

and I spent a few hours coding and

working on my own personal research

projects and I spent a lot of time

looking at the output of experiments

that I launched a few days earlier um

this question about advice for deploying

ml as a service

I don't have any broad general advice if

you want to ask a question that's like

if it is a specific aspect of ml as a

service that you're wintering about you

can ask that down below and I'll answer

it when I when I scroll down to there

somebody asks are cnn's limited in the

number of classes so you're always

limited in terms of the memory cost of

storing the weight matrix of the output

layer and if you have an extremely high

number of classes it can get expensive

to store that weight matrix there are a

lot of techniques that people use to try

to reduce that cost like using a

hierarchical softmax

rather than using one big softmax with

one big weight matrix another thing you

can do is you can factor your weight

matrix to have a bottleneck in it the

other thing that limits how well a

convolutional network can do in terms of

number of classes is just making sure

that you have training data for all

those classes you probably want to have

a few thousand examples for any class

that you care about recognizing really

accurately but the the requirement that

you have training data for all the

classes applies to pretty much every

machine learning algorithm right now

someone asks is there an LS TMR in an

algorithm for regression not

classification prediction of numerical

sequences and yeah there is one thing

that's really cool about neural networks

is it usually doesn't really matter

whether you're doing classification or

regression you just need to write down a

loss function that makes their output do

what you want it to do so anytime you

have a neural network and you want it to

output some number that you want to use

as a probability then you use that

number as the appropriate parameter of

some probability distribution so if

you're doing classification you say I'm

going to take the softmax of the output

number and that's going to give me a

distribution of your classes if you're

doing regression you say I'm going to

take the output number and I'm going to

use the output as the mean of the

Gaussian distribution and then I'm just

going to take that Gaussian distribution

measure the log likelihood of the

training data under that distribution

and I'm going to have gradient descent

minimize the negative log likelihood

someone else asks if I've played with

wet labs AI that can apparently find the

best parameters like learning rate etc

yeah so I have used wet lab on a few

different occasions back before wet lab

was called wet lab it was an open-source

product called spearmint I've written a

few different papers over the years if

anybody's followed those papers closely

the first time I tried using spearmint

was for the multi prediction deep

Boltzmann machine paper and it didn't

really work and then the next time I

tried it was for the max out networks

paper and it also didn't really work

there but let me explain a little bit

about why I think it didn't work for me

both of those papers had a lot of

different kinds of parameters that I was

trying to fit around maybe 40 different

parameters and a lot of them were for

things like the sizes of different

layers or the sizes of convolutional

network kernels or the number of layers

in the network for things like that

where you're determining the actual

architecture of the network I think that

what lab doesn't work as well I think it

probably works okay for the learning

rate and so on more recently I tried

using wet lab the actual wet lab product

for the paper that I published it I

cleared this year explained us

explaining and harnessing adversarial

examples in that case I was just using

it to choose the learning rate and

momentum parameter and in that case it

actually still didn't work very well for

me and I don't have a very good

explanation of why some of my other

friends and colleagues I do have

positive experiences with wet lab for

example George Dahl who sits right

across the table from me at Google has

used wet lab and says that it it usually

produces outputs that perform about as

well as the values he chooses himself

but he can find them with less effort if

he uses wet lab somebody asks if I think

that generative adversarial Nets have a

brighter future that variational

inference I think it's really hard to

call I think the idea of research is

people should explore many different

ideas in parallel to each other they're

also not necessarily

mutually exclusive like you could

imagine using variational inference to

recover the code in a generative

adversarial Network or you can imagine

using a generative adversarial network

to define your variational distribution

so I think every serial training and

variational techniques are both

important tools for future research

directions

okay so I've worked my way through the

question queue if any other questions

pop up I can answer them I'll hang out

for another minute or so and if there's

no new questions in I'll let you move on

thanks to everyone who's saying they

enjoyed the talk all right so it looks

like that's the end of the questions

I'll post the slides on on Google+ in

just a minute and you can just search

Google Plus for E and Goodfellow and

you'll see the link of the slides there

um and yeah have a good day everyone

you

